{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78850b5-eb23-40f7-b9c3-fda64eb8eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "读取数据并对数据做预处理\n",
    "统计出训练数据中出现频次最多的5k个单词，用这出现最多的5k个单词创建词表（词向量）\n",
    "对于测试数据，直接用训练数据构建的词表\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DataProcessor(object):\n",
    "    def read_text(self, is_train_data):\n",
    "        # 读取原始文本数据\n",
    "        # is_train_data==True表示读取训练数据\n",
    "        # is_train_data==False表示读取测试数据\n",
    "        datas = []\n",
    "        labels = []\n",
    "        if is_train_data:\n",
    "            # 训练数据目录\n",
    "            pos_path = \"./data/aclImdb/train/pos/\" \n",
    "            neg_path = \"./data/aclImdb/train/neg/\" \n",
    "        else:\n",
    "            # 测试数据目录\n",
    "            pos_path = \"./data/aclImdb/test/pos/\" \n",
    "            neg_path = \"./data/aclImdb/test/neg/\"\n",
    "        pos_files = os.listdir(pos_path)  # 获取文件夹下的所有文件名称\n",
    "        neg_files = os.listdir(neg_path)\n",
    "        for i, file_name in enumerate(pos_files):\n",
    "            if i > 2000:\n",
    "                break\n",
    "            file_position = pos_path + file_name\n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([1, 0]) # 正类标签维[1,0]\n",
    "        for i, file_name in enumerate(neg_files):\n",
    "            if(i > 2000):\n",
    "                break\n",
    "            file_position = neg_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0, 1])  # 负类标签维[0,1]\n",
    "        return datas, labels\n",
    "    \n",
    "    def word_count(self, datas):\n",
    "        # 统计单词出现的频次，并将其降序排列，得出出现频次最多的单词\n",
    "        dic = {}\n",
    "        for data in datas:\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() # 所有单词转化为小写\n",
    "                if word in dic:\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        word_count_sorted = sorted(dic.items(), \n",
    "            key=lambda item:item[1], reverse=True)\n",
    "        return word_count_sorted\n",
    "    \n",
    "    def word_index(self, datas, vocab_size):\n",
    "        # 创建词表\n",
    "        word_count_sorted = self.word_count(datas)\n",
    "        word2index = {}\n",
    "        # 词表中未出现的词\n",
    "        word2index[\"<unk>\"] = 0\n",
    "        # 句子添加的padding\n",
    "        word2index[\"<pad>\"] = 1\n",
    "        \n",
    "        # 词表的实际大小由词的数量和限定大小决定\n",
    "        vocab_size = min(len(word_count_sorted), vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            word = word_count_sorted[i][0]\n",
    "            word2index[word] = i + 2\n",
    "          \n",
    "        return word2index, vocab_size\n",
    "    \n",
    "    def get_datasets(self, vocab_size, embedding_size, max_len):\n",
    "        # 注，由于nn.Embedding每次生成的词嵌入不固定，因此此处同时获取训练数据的\n",
    "        # 词嵌入和测试数据的词嵌入\n",
    "        # 测试数据的词表也用训练数据创建\n",
    "        train_datas, train_labels = self.read_text(is_train_data=True)\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        \n",
    "        test_datas, test_labels = self.read_text(is_train_data = False)\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()  # 词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    # 词表中未出现的词用<unk>代替\n",
    "                    feature.append(word2index[\"<unk>\"]) \n",
    "                if(len(feature)==max_len): \n",
    "                    # 限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            # 对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] * \\\n",
    "                (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() # 词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    # 词表中未出现的词用<unk>代替\n",
    "                    feature.append(word2index[\"<unk>\"]) \n",
    "                if(len(feature)==max_len): \n",
    "                    # 限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            # 对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] \\\n",
    "                * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        # 将词的index转换成tensor,train_features中数据的维度需要一致\n",
    "        # 否则会报错\n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        # 将词转化为embedding\n",
    "        # 词表中有两个特殊的词<unk>和<pad>，所以词表实际大小为vocab_size + 2\n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        # 指定输入特征是否需要计算梯度\n",
    "        train_features = Variable(train_features, \n",
    "            requires_grad=False)\n",
    "        train_datasets = torch.utils.data.TensorDataset(\n",
    "            train_features, train_labels)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        test_datasets = torch.utils.data.TensorDataset(\n",
    "            test_features, test_labels)\n",
    "        return train_datasets, test_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffc0c4c-c8ea-4385-9af9-b84d91b2fda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46bfea7-ed68-4032-986d-4822060e834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    '''\n",
    "    :param input_size:词向量维度\n",
    "    :param hidden_size:隐藏单元数量\n",
    "    :param output_size:输出类别数\n",
    "    :param num_layers:RNN层数\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            self.input_size, self.hidden_size, \n",
    "            self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # 重新获取batch_size\n",
    "        h0 = torch.zeros(\n",
    "            self.num_layers, batch_size, self.hidden_size)\n",
    "        output, hidden = self.rnn(x, h0)\n",
    "        # resize使得rnn的输出结果可以输入到fc层中\n",
    "        output = output[:, -1, :]\n",
    "        # output = output.contiguous().view(-1, self.hidden_size)\n",
    "        output = self.fc(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e32d14-f204-4c52-a220-5aafde3515a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    :param embedding_size:词向量维度\n",
    "    :param hidden_size:隐藏单元数量\n",
    "    :param output_size:输出类别数\n",
    "    :param num_layers:LSTM层数\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, \n",
    "                 num_classes, device):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.num_directions = 2\n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=(self.num_directions == 2))\n",
    "\n",
    "        self.liner = nn.Linear(num_layers * self.num_directions * hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input):\n",
    "        # lstm的输入维度为 [seq_len, batch_size, input_size]\n",
    "        output = input.permute(1, 0, 2)\n",
    "        batch_size = output.size(1)\n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(self.device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(self.device)\n",
    "        out, (h_n, c_n) = self.lstm(output, (h_0, c_0))\n",
    "        output = h_n\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = output.contiguous().view(\n",
    "            batch_size,\n",
    "            self.num_layers * self.num_directions * self.hidden_size)\n",
    "        output = self.liner(output)\n",
    "        output = self.act_func(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fc077d-1c2c-4eee-9b3f-bcdfe30d16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_func, device):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels)\n",
    "\n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "\n",
    "        # 获取预测的最大概率出现的位置\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset)\n",
    "    test_acc = corrects / len(test_loader.dataset)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, device, lr):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        for datas, labels in train_loader:\n",
    "\n",
    "            preds = model(datas)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item() * datas.size(0)\n",
    "\n",
    "            # 获取预测的最大概率出现的位置\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader.dataset)\n",
    "        train_acc = corrects / len(train_loader.dataset)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(\n",
    "                train_loss, train_acc))\n",
    "            test_acc = test(model, test_loader, loss_fn, device)\n",
    "            if best_val_acc < test_acc:\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_model(config):\n",
    "    \"\"\"选择网络模型\"\"\"\n",
    "    if config.net == \"LSTM\":\n",
    "        model = LSTM(\n",
    "            config.embedding_size, config.hidden_size,\n",
    "            config.num_layers, config.num_classes, config.device)\n",
    "    elif config.net == \"RNN\":\n",
    "        model = RNN(\n",
    "            config.embedding_size, hidden_size=config.hidden_size,          \n",
    "            output_size=config.num_classes, \n",
    "            num_layers=config.num_layers)\n",
    "    else:\n",
    "        raise NameError(\"No defined net found\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataloader(config):\n",
    "    processor = DataProcessor()\n",
    "    train_datasets, test_datasets = processor.get_datasets(\n",
    "        vocab_size=config.vocab_size,\n",
    "        embedding_size=config.embedding_size,\n",
    "        max_len=config.sentence_max_len)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_datasets, batch_size=config.batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_datasets, batch_size=config.batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0084e48-9920-4063-bbd7-ceb03f734063",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/aclImdb/train/pos/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ecdfe48337ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m config.device = torch.device(config.device if torch.cuda.is_available() and config.device != 'cpu'\n",
      "\u001b[0;32m<ipython-input-4-ae259c99c074>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     78\u001b[0m     train_datasets, test_datasets = processor.get_datasets(vocab_size=config.vocab_size,\n\u001b[1;32m     79\u001b[0m                                                            \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                                                            max_len=config.sentence_max_len)\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-590974eb2e4e>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m(self, vocab_size, embedding_size, max_len)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# 注，由于nn.Embedding每次生成的词嵌入不固定，因此此处同时获取训练数据的词嵌入和测试数据的词嵌入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# 测试数据的词表也用训练数据创建\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtrain_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mword2index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-590974eb2e4e>\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, is_train_data)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpos_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/aclImdb/test/pos/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mneg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/aclImdb/test/neg/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpos_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取文件夹下的所有文件名称\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mneg_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/aclImdb/train/pos/'"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"config\")\n",
    "    parser.add_argument('--net', default='RNN', type=str)\n",
    "    # parser.add_argument('--dataset', default=None, type=str)\n",
    "    parser.add_argument('--vocab_size', default=10000, type=int)\n",
    "    parser.add_argument('--embedding_size', default=100, type=int)\n",
    "    parser.add_argument('--num_classes', default=2, type=int)\n",
    "    parser.add_argument('--sentence_max_len', default=64, type=int)\n",
    "    parser.add_argument('--num_layers', default=4, type=int)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "    parser.add_argument('--num_epochs', default=256, type=int)\n",
    "    parser.add_argument('--hidden_size', default=32, type=int)\n",
    "    parser.add_argument('--device', default='cpu', type=str)\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "config = parse_args()\n",
    "train_loader, test_loader = get_dataloader(config)\n",
    "model = select_model(config)\n",
    "config.device = torch.device(config.device if torch.cuda.is_available() and config.device != 'cpu'\n",
    "                             else 'cpu')\n",
    "model = model.to(config.device)\n",
    "model = train(model, train_loader, test_loader, \n",
    "              config.num_epochs, config.device, config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4ba8c-bd7e-4e23-aeef-2a02c9fe0f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
