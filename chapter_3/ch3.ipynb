{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e75bc57",
   "metadata": {},
   "source": [
    "# 第3章 深度学习计算框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e13a3",
   "metadata": {},
   "source": [
    "## 3.4 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1f156",
   "metadata": {},
   "source": [
    "**1. 张量的创建**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40684ef",
   "metadata": {},
   "source": [
    "1. 以Python列表形式创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8525fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_list = torch.tensor(data)\n",
    "print(x_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141744f1",
   "metadata": {},
   "source": [
    "2. 以numpy数组形式创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1a8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array = np.array(data, dtype=float)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1b605",
   "metadata": {},
   "source": [
    "3. 以指定形状、常量创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.4492, 0.1088, 0.9981],\n",
      "        [0.5390, 0.9038, 0.0843]])\n",
      "tensor([[-0.6765, -0.7505, -0.2299],\n",
      "        [-0.4672,  0.1520,  0.4656]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(2, 3)\n",
    "x_zeros = torch.zeros(2, 3)\n",
    "x_rand = torch.rand(2, 3)\n",
    "x_randn = torch.randn(2, 3)\n",
    "\n",
    "print(x_ones)\n",
    "print(x_zeros)\n",
    "print(x_rand)\n",
    "print(x_randn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704bb5f",
   "metadata": {},
   "source": [
    "4. 根据已有张量创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033d6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones_like = torch.ones_like(x_list)\n",
    "x_zeros_like = torch.zeros_like(x_list)\n",
    "\n",
    "# x_list为整型数据，与均匀分布和标准分布需要的浮点型数据冲突\n",
    "x_rand_like = torch.rand_like(x_list, dtype=torch.float)  # 以dtype参数重载数据类型\n",
    "x_randn_like = torch.randn_like(x_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e86fb",
   "metadata": {},
   "source": [
    "5. 其他常用张量创建方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be6418",
   "metadata": {},
   "source": [
    "torch.full(size, value)根据输入的形状和值来创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04c9e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1400, 3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "x_full = torch.full([3, 4], 3.14)\n",
    "print(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc5fa1",
   "metadata": {},
   "source": [
    "torch.arange(start, end, step)将创建一个左闭右开区间[start, end)内的一维张量，其步长为step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fec006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x_arange = torch.arange(1, 10, 1)\n",
    "print(x_arange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc11053",
   "metadata": {},
   "source": [
    "torch.linspace(start, end, steps)将创建一个闭区间[start, end]内的一维向量，其中的元素个数为steps，呈等间隔分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d5bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.5000,  1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,\n",
      "         4.0000,  4.5000,  5.0000,  5.5000,  6.0000,  6.5000,  7.0000,  7.5000,\n",
      "         8.0000,  8.5000,  9.0000,  9.5000, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "x_linspace = torch.linspace(0, 10, 21)\n",
    "print(x_linspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999de282",
   "metadata": {},
   "source": [
    "**2.张量的数据类型、类型转换**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8c165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int8)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(2, 3)\n",
    "x_ones_8bit_int = x_ones.type(torch.CharTensor)\n",
    "x_ones_32bit_int = x_ones.type(torch.IntTensor)\n",
    "\n",
    "print(x_ones_8bit_int)\n",
    "print(x_ones_32bit_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a9751",
   "metadata": {},
   "source": [
    "PyTorch也支持在定义张量时直接通过dtype参数指定数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebec644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.CharTensor\n",
      "torch.ShortTensor\n",
      "torch.IntTensor\n",
      "torch.LongTensor\n",
      "torch.HalfTensor\n",
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "x_ones_int8 = torch.ones(2, 3, dtype=torch.int8)\n",
    "x_ones_int16 = torch.ones(2, 3, dtype=torch.int16)\n",
    "x_ones_int32 = torch.ones(2, 3, dtype=torch.int32)\n",
    "x_ones_int64 = torch.ones(2, 3, dtype=torch.int64)\n",
    "x_ones_float16 = torch.ones(2, 3, dtype=torch.float16)\n",
    "x_ones_float32 = torch.ones(2, 3, dtype=torch.float32)\n",
    "x_ones_float64 = torch.ones(2, 3, dtype=torch.float64)\n",
    "\n",
    "print(x_ones_int8.type())\n",
    "print(x_ones_int16.type())\n",
    "print(x_ones_int32.type())\n",
    "print(x_ones_int64.type())\n",
    "print(x_ones_float16.type())\n",
    "print(x_ones_float32.type())\n",
    "print(x_ones_float64.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b81f3c",
   "metadata": {},
   "source": [
    "PyTorch张量与Numpy数组的互相转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7b01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([3, 2],dtype=np.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a7ab1",
   "metadata": {},
   "source": [
    "Numpy转化成Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20f8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "b = torch.Tensor(a) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d26ca",
   "metadata": {},
   "source": [
    "Tensor 转化为 Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "850d22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "c = b.numpy()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c938f85",
   "metadata": {},
   "source": [
    "Numpy和Tensor共享内存的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596269bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., 10.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "[[ 1. 10.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "a[0, 1]=10     # 将10赋值给第一行第二列元素赋值\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f37e8f",
   "metadata": {},
   "source": [
    "注意： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef08b32",
   "metadata": {},
   "source": [
    "CPU张量与GPU张量的互相转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b134abd",
   "metadata": {},
   "source": [
    "默认情况下，张量会创建在CPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf56a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c8eb2",
   "metadata": {},
   "source": [
    "创建GPU张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36ca2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): # 如果支持GPU加速\n",
    "    a = torch.randn(2,3, device=torch.device('cuda:0'))\n",
    "   \t# 等价于\n",
    "    # a.torch.randn(2,3).cuda(0)\n",
    "    # 但是前者更快\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1ad07",
   "metadata": {},
   "source": [
    "GPU张量和CPU张量相互转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2db4e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "b =a.to(device)\n",
    "print(b.device)\n",
    "device = torch.device('cuda:0')\n",
    "c =b.to(device)\n",
    "print(c.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485006a",
   "metadata": {},
   "source": [
    "**3.张量的形状调整操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3625aa",
   "metadata": {},
   "source": [
    "通过tensor.view方法可以调整Tensor的形状，但必须保证调整前后元素总数一致。view不会修改自身的数据，返回的新Tensor与源Tensor共享内存，也即更改其中的一个，另外一个也会跟着改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c558e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 8)\n",
    "print(a.view(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57a1a9",
   "metadata": {},
   "source": [
    "tensor.view当某一维为-1的时候，会自动计算它的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39bc056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "b = a.view(-1, 4) \n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd59c53",
   "metadata": {},
   "source": [
    "由于a和b共享内存，因此a修改，b也会跟着修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efd473c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 10,  2,  3],\n",
      "        [ 4,  5,  6,  7]])\n"
     ]
    }
   ],
   "source": [
    "a[1] = 10\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b332c8",
   "metadata": {},
   "source": [
    "避免内存共享可采取拷贝方法，torch.clone()函数可以返回一个完全相同的Tensor，新的Tensor将开辟新的内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcfa89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 10,  2,  3],\n",
      "        [ 4,  5,  6,  7]])\n"
     ]
    }
   ],
   "source": [
    "c= b.clone()\n",
    "b[1]= 1  # 改变b，不会发生改变\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f061c10",
   "metadata": {},
   "source": [
    "在实际应用中可能经常需要添加或减少某一维度，可以使用squeeze和unsqueeze两个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97943b46",
   "metadata": {},
   "source": [
    "unsqueeze操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e680a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "c = b.unsqueeze(dim=1) #在第1维（下标从0开始）上增加“１”\n",
    "#等价于 b[:,None]\n",
    "print(c.shape)\n",
    "d = b.unsqueeze(-1) # -1表示倒数第一个维度\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a025d",
   "metadata": {},
   "source": [
    "squeeze操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1cdee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0, 10,  2,  3],\n",
      "           [ 1,  1,  1,  1]]]]])\n",
      "tensor([[[[ 0, 10,  2,  3],\n",
      "          [ 1,  1,  1,  1]]]])\n",
      "tensor([[ 0, 10,  2,  3],\n",
      "        [ 1,  1,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "# 注意每次压缩的形状\n",
    "c = b.unsqueeze(dim=1) #在第1维（下标从0开始）上增加“１”\n",
    "c = b.view(1, 1, 1, 2, 4)\n",
    "print(c)\n",
    "d = c.squeeze(0) # 压缩第0维的“１”\n",
    "print(d)\n",
    "d = c.squeeze() # 把所有维度为“1”的压缩\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f30fd4",
   "metadata": {},
   "source": [
    "resize是另一种可用来调整size的方法，但与view不同，它可以修改Tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "334ccf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 10,  2,  3]])\n",
      "tensor([[              0,              10,               2,               3],\n",
      "        [              1,               1,               1,               1],\n",
      "        [             25,  93970119698776, 140551396087024,               1]])\n"
     ]
    }
   ],
   "source": [
    "c = b.resize_(1, 4)\n",
    "print(c)\n",
    "c = b.resize_(3, 4) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db24782",
   "metadata": {},
   "source": [
    "repeat方法会根据给定的size，每个维度扩展size对应数，得到新的Tensor张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "416642c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([9, 4])\n",
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 3).view(3,1)\n",
    "print(a.shape)\n",
    "b = a.repeat(3,4)\n",
    "c = a.repeat(2,3,4)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b6348",
   "metadata": {},
   "source": [
    "Tensor张量的维度变换可以采用permute方法和transpose方法。transpose(input, dim0, dim1) 交换给定的dim0和dim1。Permute()可以一次操作多个维度，但每次操作必须传入所有维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b841765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 6).view(2,1,3)\n",
    "b = torch.transpose(a,0,1)    \n",
    "# 注意没有torch.permute,只有x.permute()\n",
    "c = a.permute(2,0,1)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58030762",
   "metadata": {},
   "source": [
    "注意：permute方法和transpose方法均返回原Tensor的view。即改变其中一个值，也会导致另一个值改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09fb19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,   1,   2]],\n",
      "\n",
      "        [[100,   4,   5]]])\n",
      "tensor([[[  0,   1,   2],\n",
      "         [100,   4,   5]]])\n",
      "tensor([[[  0],\n",
      "         [100]],\n",
      "\n",
      "        [[  1],\n",
      "         [  4]],\n",
      "\n",
      "        [[  2],\n",
      "         [  5]]])\n"
     ]
    }
   ],
   "source": [
    "c[0,1]=100\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fb8d5",
   "metadata": {},
   "source": [
    "**4.张量的逐元素操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7b168",
   "metadata": {},
   "source": [
    "逐元素操作会对Tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致，对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如a ** 2 等价于torch.pow(a,2), a * 2等价于torch.mul(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "748c3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000,  0.5403, -0.4161, -0.9900, -0.6536,  0.2837])\n",
      "tensor([0., 1., 0., 1., 0., 1.])\n",
      "tensor([  0.,   1.,   8.,  27.,  64., 125.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 6).float()\n",
    "print(torch.cos(a))\n",
    "print(a %  2) # 等价于t.fmod(a, 3)\n",
    "print(a ** 3) # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a03bf",
   "metadata": {},
   "source": [
    "**5.张量的归并操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305d0bf",
   "metadata": {},
   "source": [
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法sum，既可以计算整个Tensor的和，也可以计算Tensor中每一行或每一列的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4a26f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.]])\n",
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones(3, 2)\n",
    "c= b.sum(dim = 0, keepdim=True)\n",
    "print(c)\n",
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "c= b.sum(dim=0, keepdim=False)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8784a0",
   "metadata": {},
   "source": [
    "cumsum则可以沿着行累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "006b2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 0,  1,  3],\n",
      "        [ 3,  7, 12]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "b = a.cumsum(dim=1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cb41b",
   "metadata": {},
   "source": [
    "**6.张量的索引操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdac51",
   "metadata": {},
   "source": [
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，如无特殊说明，索引出来的结果与原tensor共享内存，即修改一个，另一个会跟着修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c01f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9428,  1.5430,  1.2373, -1.4099],\n",
      "        [-0.8818, -0.0888,  0.9404,  0.3046],\n",
      "        [ 0.9807,  1.5769, -0.3019,  0.6895]])\n",
      "tensor([-0.9428,  1.5430,  1.2373, -1.4099])\n",
      "tensor([-0.9428, -0.8818,  0.9807])\n",
      "tensor(1.2373)\n",
      "tensor(-1.4099)\n",
      "tensor([[-0.9428,  1.5430,  1.2373, -1.4099],\n",
      "        [-0.8818, -0.0888,  0.9404,  0.3046]])\n",
      "tensor([[-0.9428,  1.5430],\n",
      "        [-0.8818, -0.0888]])\n",
      "tensor([[-0.9428,  1.5430]])\n",
      "tensor([-0.9428,  1.5430])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "\n",
    "print(a)\n",
    "print(a[0]) # 第0行(下标从0开始)\n",
    "print(a[:, 0]) # 第0列\n",
    "print(a[0][2]) # 第0行第2个元素，等价于a[0, 2]\n",
    "print(a[0, -1]) # 第0行最后一个元素\n",
    "print(a[:2]) # 前两行\n",
    "print(a[:2, 0:2])# 前两行，第0,1列\n",
    "print(a[0:1, :2]) # 第0行，前两列\n",
    "print(a[0, :2]) # 注意上一个print的区别：形状不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2e536",
   "metadata": {},
   "source": [
    "None类似于np.newaxis, 为a新增了一个轴，等价于a.view(1, a.shape[0], a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76a0a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 1, 4, 1, 1])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor([])\n",
      "tensor([[ 0.9719, -1.2487,  0.6139,  0.4218],\n",
      "        [-0.8369,  0.2941, -1.6502, -1.7448]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "\n",
    "print(a[None].shape)\n",
    "print(a[None].shape )# 等价于a[None,:,:]\n",
    "print(a[:,None,:].shape)\n",
    "print(a[:,None,:,None,None].shape)\n",
    "print(a > 1 )# 返回一个ByteTensor\n",
    "print(a[a>1] )# 等价于a.masked_select(a>1), 选择结果与原tensor不共享内存空间\n",
    "print(a[torch.LongTensor([0,1])]) # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc006f9",
   "metadata": {},
   "source": [
    "gather作为一个比较复杂的操作，三维tensor的gather操作如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcf57e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "tensor([[ 0,  5, 10, 15]])\n",
      "tensor([[ 3],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [12]])\n",
      "tensor([[12,  9,  6,  3]])\n",
      "tensor([[ 0,  3],\n",
      "        [ 5,  6],\n",
      "        [10,  9],\n",
      "        [15, 12]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 16).view(4, 4)\n",
    "index1 = torch.LongTensor([[0,1,2,3]])\n",
    "index2 = torch.LongTensor([[3,2,1,0]]).t()\n",
    "index3 = torch.LongTensor([[3,2,1,0]])\n",
    "index4 = torch.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "\n",
    "print(a)\n",
    "print(a.gather(0, index1))# 选取对角线的元素\n",
    "print(a.gather(1, index2))# 选取反对角线上的元素\n",
    "print(a.gather(0, index3))# 选取反对角线上的元素，注意与上面的不同\n",
    "print(a.gather(1, index4))# 选取两个对角线上的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519baac7",
   "metadata": {},
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用tensor.item(), 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90b5fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "0\n",
      "torch.Size([1, 1, 1])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 16).view(4, 4)\n",
    "d = a[0:1, 0:1, None]\n",
    "\n",
    "print(a[0,0])#依旧是tensor\n",
    "print(a[0,0].item())# python float\n",
    "print(d.shape)\n",
    "print(d.item()) # 只包含一个元素的tensor即可调用tensor.item,与形状无关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111fced",
   "metadata": {},
   "source": [
    "PyTorc支持绝大多数numpy的高级索引。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93820ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n",
      "tensor([14, 24])\n",
      "tensor([19, 10,  1])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,27).view(3,3,3)\n",
    "\n",
    "print(x)\n",
    "print(x[[1, 2], [1, 2], [2, 0]]) # x[1,1,2]和x[2,2,0]\n",
    "print(x[[2, 1, 0], [0], [1]])# x[2,0,1],x[1,0,1],x[0,0,1]\n",
    "print(x[[0, 2], ...])# x[0] 和 x[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459d105",
   "metadata": {},
   "source": [
    "**7.张量的比较操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572067",
   "metadata": {},
   "source": [
    "比较一个tensor和一个数，可以使用clamp函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3f100f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  3.,  6.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[15., 12.,  9.],\n",
      "        [ 6.,  3.,  0.]])\n",
      "tensor([[False, False, False],\n",
      "        [ True,  True,  True]])\n",
      "tensor([ 9., 12., 15.])\n",
      "tensor(15.)\n",
      "torch.return_types.max(\n",
      "values=tensor([15.,  6.]),\n",
      "indices=tensor([0, 0]))\n",
      "tensor([[15., 12.,  9.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[10., 10., 10.],\n",
      "        [10., 12., 15.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0, 15, 6).view(2, 3)\n",
    "b = torch.linspace(15, 0, 6).view(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(a>b)\n",
    "print(a[a>b]) # a中大于b的元素\n",
    "print(torch.max(a))\n",
    "print(torch.max(b, dim=1))\n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# #第二个返回值的0和0表示上述最大的数是该行第0个元素\n",
    "print(torch.max(a,b))\n",
    "print(torch.clamp(a, min=10))# 比较a和10较大的元素 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2e008",
   "metadata": {},
   "source": [
    "**8.张量的线性代数操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282489e",
   "metadata": {},
   "source": [
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6df53d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor(15)\n",
      "tensor([1, 5, 9])\n",
      "tensor([[1, 2, 3],\n",
      "        [0, 5, 6],\n",
      "        [0, 0, 9]])\n",
      "tensor([[0, 2, 3],\n",
      "        [0, 0, 6],\n",
      "        [0, 0, 0]])\n",
      "tensor([[1, 0, 0],\n",
      "        [4, 5, 0],\n",
      "        [7, 8, 9]])\n",
      "tensor([[0, 0, 0],\n",
      "        [4, 0, 0],\n",
      "        [7, 8, 0]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(a)\n",
    "print(torch.trace(a)) # 求矩阵a的迹\n",
    "print(torch.diag(a)) # 求矩阵a的对角线元素\n",
    "print(torch.triu(a)) # 求矩阵a的上三角矩阵，无偏移量\n",
    "print(torch.triu(a, diagonal=1)) # 求矩阵a的上三角矩阵，偏移量为1\n",
    "print(torch.tril(a)) # 求矩阵a的下三角矩阵，无偏移量\n",
    "print(torch.tril(a, diagonal=-1)) # 求矩阵a的下三角矩阵，偏移量为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbf5e7",
   "metadata": {},
   "source": [
    "bmm函数用于计算批量矩阵的乘法，即同时批量组矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b32659e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0723, -0.4299,  0.1832, -0.4865, -0.7040],\n",
      "         [-0.7577, -0.9718,  0.1325, -2.1047, -1.4561]],\n",
      "\n",
      "        [[-1.7467, -0.2075, -0.1490, -1.0882,  0.5957],\n",
      "         [ 0.8632,  0.3368,  0.7400,  0.5647, -2.2321]]])\n",
      "tensor([[[-0.0723, -0.4299],\n",
      "         [ 0.1832, -0.4865],\n",
      "         [-0.7040, -0.7577],\n",
      "         [-0.9718,  0.1325],\n",
      "         [-2.1047, -1.4561]],\n",
      "\n",
      "        [[-1.7467, -0.2075],\n",
      "         [-0.1490, -1.0882],\n",
      "         [ 0.5957,  0.8632],\n",
      "         [ 0.3368,  0.7400],\n",
      "         [ 0.5647, -2.2321]]])\n",
      "tensor([[[ 1.7520,  1.0620],\n",
      "         [ 4.8934,  2.5395]],\n",
      "\n",
      "        [[ 2.9631, -1.6752],\n",
      "         [-2.1874,  5.4933]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.randn((2,2,5)) # 批量大小为2，即两组2*5的矩阵\n",
    "d = torch.reshape(c,(2,5,2)) # 两组5*2的矩阵\n",
    "\n",
    "print(c)\n",
    "print(d)\n",
    "print(torch.bmm(c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf213e",
   "metadata": {},
   "source": [
    "需要注意，矩阵的转置会导致存储空间不连续，torch.view等方法操作需要连续的Tensor，导致如果按照语义的形状进行view拉伸，数字会不连续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59a1c867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 3],\n",
      "        [4, 5, 9]])\n",
      "tensor([[2, 1, 3, 4, 5, 9]])\n",
      "tensor([[2, 4],\n",
      "        [1, 5],\n",
      "        [3, 9]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-40cd8f9ae802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[2, 1, 3], [4, 5, 9]])\n",
    "t1 = t.t()\n",
    "print(t)\n",
    "print(t.view(1,6))\n",
    "print(t1)\n",
    "print(t1.view(1,6)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25514c51",
   "metadata": {},
   "source": [
    "此时需调用它的.contiguous方法将其转为连续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b376e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4],\n",
      "        [1, 5],\n",
      "        [3, 9]])\n",
      "tensor([[2, 4, 1, 5, 3, 9]])\n"
     ]
    }
   ],
   "source": [
    "t2 = t1.contiguous()\n",
    "\n",
    "print(t2)\n",
    "print(t2.view(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b228eb",
   "metadata": {},
   "source": [
    "## 3.5 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48be6b8",
   "metadata": {},
   "source": [
    "使用PyTorch来实现单层感知机的例子，并进一步分析PyTorch的自动求导机制，首先需要使用PyTorch来表示上述计算过程，设置参数requires_grad=True，在计算图中指定变量需要计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d52937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # 输入x: [1, 1, 1, 1, 1]\n",
    "y = torch.zeros(3)  # 标签y: [0, 0, 0]\n",
    "w = torch.randn(5, 3, requires_grad=True) # 使用标准分布初始化参数矩阵w\n",
    "b = torch.randn(3, requires_grad=True) # 使用标准分布初始化参数向量b\n",
    "z = torch.matmul(x, w)+b  # 中间变量z=x*w+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # 求交叉熵损失值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523afb08",
   "metadata": {},
   "source": [
    "可以使用PyTorch的grad_fn函数来查看中间变量的梯度函数类型，它将记录中间变量是由什么操作得到输出的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09b2427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7fd3bac85a90>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fd3bac85e50>\n",
      "Gradient function for w = None\n",
      "Gradient function for b = None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
    "print(f\"Gradient function for w = {w.grad_fn}\")  # 未指定需要计算梯度\n",
    "print(f\"Gradient function for b = {b.grad_fn}\")  # 未指定需要计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fac91",
   "metadata": {},
   "source": [
    "可以看到，对于计算过程中的中间变量z和最终输出loss，PyTorch会自动地记录它们所参与的运算。而对于用户定义初始化的变量w，b，它们称为叶子节点，其对应的梯度函数为None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c59c56",
   "metadata": {},
   "source": [
    "PyTorch提供了backward()函数，用以自动地求解所有requires_grad=True的张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a83e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1011, 0.2972, 0.0466],\n",
      "        [0.1011, 0.2972, 0.0466],\n",
      "        [0.1011, 0.2972, 0.0466],\n",
      "        [0.1011, 0.2972, 0.0466],\n",
      "        [0.1011, 0.2972, 0.0466]])\n",
      "tensor([0.1011, 0.2972, 0.0466])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db30ca8",
   "metadata": {},
   "source": [
    "## 3.6神经网络层和模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283bef8",
   "metadata": {},
   "source": [
    "在PyTorch中，torch.nn中封装了大量可供直接调用的模块，包括常用的全连接层、卷积层（Convolutional Layer）、池化层（Pooling Layer）等神经网络基础模块，也包括各种激活函数、损失函数等常用的数学运算操作，用户可以直接调用这些模块完整地搭建各种神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c027ee",
   "metadata": {},
   "source": [
    "1. **torch.nn.Flatten**： 使用torch.nn.Flatten()可以定义一个展开层，它可以将输入的高维张量展开为1维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a06941c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16384])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,128,128) #假设输入3个128*128的矩阵\n",
    "flatten = torch.nn.Flatten() # 定义一个展平层\n",
    "flat_image = flatten(input_image) # 使用展平层将矩阵展开成向量\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1b484",
   "metadata": {},
   "source": [
    "2. **torch.nn.Linear**：使用torch.nn.Linear(in_features, out_features)可以直接定义一个输入和输出分别为in_features和out_features的全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98808e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "fc = torch.nn.Linear(128*128, 20) # 定义一个128*128到20的全连接层\n",
    "output = fc(flat_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b965de4",
   "metadata": {},
   "source": [
    "3. **torch.nn.ReLU**：使用torch.nn.ReLU()可以定义一个ReLU激活函数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49461817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3538, 0.4886, 0.0000, 0.5667, 0.0616, 0.0000, 0.4835, 0.0000,\n",
      "         0.0000, 0.0000, 0.0243, 0.0591, 0.0000, 0.3148, 0.3913, 0.4390, 0.0000,\n",
      "         0.0000, 0.4294],\n",
      "        [0.0000, 0.4610, 0.4736, 0.1083, 0.3916, 0.0000, 0.0000, 0.6451, 0.0000,\n",
      "         0.0000, 0.0000, 0.4695, 0.0000, 0.0719, 0.2537, 0.1805, 0.5672, 0.1406,\n",
      "         0.0000, 0.3099],\n",
      "        [0.0074, 0.1678, 0.2357, 0.0830, 0.0547, 0.3910, 0.0000, 0.5839, 0.0000,\n",
      "         0.0000, 0.0638, 0.2261, 0.0000, 0.0000, 0.5542, 0.0604, 0.4876, 0.0747,\n",
      "         0.0000, 0.2898]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "output = relu(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b90298",
   "metadata": {},
   "source": [
    "4. **torch.nn.Sequence**：orch.nn.Sequence()是一个有序的模块容器，它能够按顺序封装多个模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cdd763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(128*128, 20),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "input_image = torch.rand(3,128,128)\n",
    "output = seq_modules(input_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e76b6",
   "metadata": {},
   "source": [
    "5. **torch.optim**：PyTorch的optim模块内封装了一系列的优化方法，也称为优化器（optimizer），例如torch.optim.SGD（随机梯度下降）、torch.optim.Adagrad、torch.optim.Adam等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46d01e",
   "metadata": {},
   "source": [
    "## 3.7 PyTorch神经网络学习实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0a1a6",
   "metadata": {},
   "source": [
    "**1.模型定义**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73ad08",
   "metadata": {},
   "source": [
    "在PyTorch中，torch.nn.Module是所有的网络模块和神经网络模型的基类，用户在自定义模型时，也需要继承该基类，同时必须定义以下两个函数：\n",
    "\n",
    "（1）\\_\\_init\\_\\_()：构造函数，定义模型所需要用的神经网络层，并完成模型的初始化；\n",
    "\n",
    "（2）forward()：前向传播函数，定义输入数据前向传播的计算过程。\n",
    "\n",
    "这里以3层全连接层神经网络的定义为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "151674a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    # 构造函数，定义网络中的模块\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        # 展开层，将输入图片展开为一维向量\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 3层全连接层\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),   # 输入层：将输入映射到长度为512\n",
    "            nn.ReLU(),               # ReLU激活函数\n",
    "            nn.Linear(512, 512),     # 隐藏层\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),      # 输出层：将特征向量映射到10个类别\n",
    "        )\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # 展开图片\n",
    "        outputs = self. linear_layers(x) # 经过全连接层得到输出\n",
    "        return outputs\n",
    "# 实例化模型\n",
    "model = FullyConnectedNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08e7dd",
   "metadata": {},
   "source": [
    "**2.载入数据**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88abe4",
   "metadata": {},
   "source": [
    "使用torchvision库可以直接下载并导入MNIST数据集，并使用PyTorch的DataLoader模块将数据集处理成神经网络所需的批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55c1f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "# 定义训练集\n",
    "training_data = datasets.MNIST(\n",
    "    root= \"../chapter_2\",  # 所用mnist数据集已在第二章中下载过\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 定义测试集\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"../chapter_2\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 设置批量大小为64\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d57356",
   "metadata": {},
   "source": [
    "**3.定义超参数**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277a996",
   "metadata": {},
   "source": [
    "在开始训练神经网络前，需要定义以下几个主要的超参数:\n",
    "1. 训练轮数（epochs）\n",
    "2. 批量大小（batch_size）\n",
    "3. 学习率（learning_rate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdb33401",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790910ce",
   "metadata": {},
   "source": [
    "**4.定义损失函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e3fbd",
   "metadata": {},
   "source": [
    "这里使用第二章所介绍的常用分类损失函数——交叉熵损失（Cross Entropy Loss），在PyTorch中，可以直接调用nn.CrossEntropyLoss()模块来定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93677b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58abea9f",
   "metadata": {},
   "source": [
    "**5.定义优化器**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbcb24",
   "metadata": {},
   "source": [
    "这里使用第二章所介绍的随机梯度下降优化方法作为优化器，在PyTorch中，可以直接调用torch.optim.SGD()模块来定义，这里需要传入两个参数（1）模型参数（2）学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17435109",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251f581",
   "metadata": {},
   "source": [
    "**6.定义单轮训练函数及单轮测试函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff84dc",
   "metadata": {},
   "source": [
    "一般情况下，可以先定义出单轮的训练及测试函数，在这里只考虑遍历一次数据集所需要完成的操作，主要包括前向传播、梯度反向传播、梯度更新和打印相关指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c6bdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单轮训练函数\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # 遍历训练集\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 前向传播：直接向实例化后的模型传入数据\n",
    "        pred = model(X)\n",
    "\n",
    "        # 计算损失函数值\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 反向传播前将优化器的梯度清空\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 梯度反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 优化器对模型内的参数进行更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每100个批量样本打印一次损失函数值\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(\"loss: %.5f  [%d/%d]\" % (loss, current, size))\n",
    "\n",
    "# 单轮测试函数\n",
    "def test_one_epoch(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # 初始化测试指标\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # 关闭梯度记录\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # 计算测试指标并打印\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\"Accuracy: %.2f, Avg loss: %.5f \\n\" % (correct, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce46659",
   "metadata": {},
   "source": [
    "**7.完整训练流程**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95b805",
   "metadata": {},
   "source": [
    "在完整的训练流程中，只需要执行对应轮数次的循环，每个循环内执行一次单轮训练和单轮测试（也可以多轮训练后测试一次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba18adf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.30868  [0/60000]\n",
      "loss: 2.30138  [6400/60000]\n",
      "loss: 2.28546  [12800/60000]\n",
      "loss: 2.29151  [19200/60000]\n",
      "loss: 2.28362  [25600/60000]\n",
      "loss: 2.27611  [32000/60000]\n",
      "loss: 2.27325  [38400/60000]\n",
      "loss: 2.27761  [44800/60000]\n",
      "loss: 2.26591  [51200/60000]\n",
      "loss: 2.25338  [57600/60000]\n",
      "Accuracy: 0.44, Avg loss: 2.25552 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.26061  [0/60000]\n",
      "loss: 2.25077  [6400/60000]\n",
      "loss: 2.24649  [12800/60000]\n",
      "loss: 2.22963  [19200/60000]\n",
      "loss: 2.23356  [25600/60000]\n",
      "loss: 2.22571  [32000/60000]\n",
      "loss: 2.21154  [38400/60000]\n",
      "loss: 2.23173  [44800/60000]\n",
      "loss: 2.20441  [51200/60000]\n",
      "loss: 2.18678  [57600/60000]\n",
      "Accuracy: 0.63, Avg loss: 2.18833 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.19347  [0/60000]\n",
      "loss: 2.17783  [6400/60000]\n",
      "loss: 2.18728  [12800/60000]\n",
      "loss: 2.13683  [19200/60000]\n",
      "loss: 2.15387  [25600/60000]\n",
      "loss: 2.14445  [32000/60000]\n",
      "loss: 2.11160  [38400/60000]\n",
      "loss: 2.15391  [44800/60000]\n",
      "loss: 2.10255  [51200/60000]\n",
      "loss: 2.07470  [57600/60000]\n",
      "Accuracy: 0.68, Avg loss: 2.07383 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.07956  [0/60000]\n",
      "loss: 2.05188  [6400/60000]\n",
      "loss: 2.08140  [12800/60000]\n",
      "loss: 1.97945  [19200/60000]\n",
      "loss: 2.01134  [25600/60000]\n",
      "loss: 1.99689  [32000/60000]\n",
      "loss: 1.93831  [38400/60000]\n",
      "loss: 2.01358  [44800/60000]\n",
      "loss: 1.92526  [51200/60000]\n",
      "loss: 1.88157  [57600/60000]\n",
      "Accuracy: 0.72, Avg loss: 1.87392 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.88483  [0/60000]\n",
      "loss: 1.83462  [6400/60000]\n",
      "loss: 1.89098  [12800/60000]\n",
      "loss: 1.72789  [19200/60000]\n",
      "loss: 1.76583  [25600/60000]\n",
      "loss: 1.74189  [32000/60000]\n",
      "loss: 1.66445  [38400/60000]\n",
      "loss: 1.78458  [44800/60000]\n",
      "loss: 1.65296  [51200/60000]\n",
      "loss: 1.59505  [57600/60000]\n",
      "Accuracy: 0.75, Avg loss: 1.57559 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(\"Epoch %d\\n-------------------------------\" % (t+1))\n",
    "    train_one_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_one_epoch(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c6265",
   "metadata": {},
   "source": [
    "**8.模型参数保存及调用**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e3bd9",
   "metadata": {},
   "source": [
    "当模型训练结束后，可以将模型保存至PyTorch的pth文件中，后续就可以无需训练直接调用文件中的已训练模型进行测试。\n",
    "使用torch.save()函数可以将模型保存至pth文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "298fcd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d91a3d",
   "metadata": {},
   "source": [
    "使用torch.load()函数可以将模型从pth中读取出来，这时模型中的参数已经训练过，可以直接调用测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c9b0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75, Avg loss: 1.57559 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_new = torch.load('model.pth')\n",
    "\n",
    "test_one_epoch(test_dataloader, model_new, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9cc8f",
   "metadata": {},
   "source": [
    "可以看到，输出了与前面已训练模型一致的测试指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4a6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
